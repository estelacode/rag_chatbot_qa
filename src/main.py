# Autor: Estela Madariaga

import os
import sys
import gradio as gr
from src.config import PLACE_HOLDER, BOT_ICON, USER_ICON, RAG_PROMPT
from dotenv import load_dotenv,find_dotenv
from langchain_cohere.llms import Cohere
from langchain_cohere import ChatCohere
from langchain_cohere.embeddings import CohereEmbeddings
from langchain_community.vectorstores import FAISS
from langchain_core.prompts import PromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain.schema.runnable import RunnableMap
from langchain_core.runnables import RunnablePassthrough
import logging


# Configurar el logging
logging.basicConfig(
    level=logging.DEBUG,  # Nivel de log (DEBUG, INFO, WARNING, ERROR, CRITICAL)
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',  # Formato del log
    datefmt='%Y-%m-%d %H:%M:%S',  # Formato de la fecha
    filename="app.log",  # Archivo de log.
    filemode='a'  # Modo de apertura del archivo (a: append, w: write)
)

# Obtener un logger
logger = logging.getLogger(__name__)

# Cargar variables de entorno desde el archivo .env
load_dotenv(".env")



def rag_respond(message, history):
    """ 
    Responds to a user's message by invoking a RAG chain.

    Args:
        message (str): The user's message to be responded to.
        history (str): The conversation history.

    Returns:
        str: The response content generated by the LLM model.
    """
    embeddings = CohereEmbeddings( model="embed-multilingual-v3.0",cohere_api_key=os.getenv("COHERE_API_KEY"))

    # Cargamos la base de datos vectorial creada en la fase de ingestion
    vectorstore = FAISS.load_local(
        "./data/faiss_index", embeddings, allow_dangerous_deserialization=True
    )

   
    # Buscamos los documentos relevantes
    retriever = vectorstore.as_retriever(search_kwargs={"k": 1})
   
    # prompt 
    prompt = PromptTemplate.from_template(RAG_PROMPT)

    # Instanciamos el modelo de lenguaje
    llm = ChatCohere(cohere_api_key=os.getenv("COHERE_API_KEY"))

    # RAG chain
    rag_chain = (
        {"context": retriever, "query": RunnablePassthrough()}
        | prompt
        | llm
        | StrOutputParser()
    )

    # invoke rag chain
    return rag_chain.invoke( message)


# Define chatbot method
def respond(message, history):
    """
    Responds to a user's message by invoking a ChatCohere LLM model.

    Args:
        message (str): The user's message to be responded to.
        history (str): The conversation history.

    Returns:
        str: The response content generated by the LLM model.
    """
    llm = ChatCohere()
    response = llm.invoke(message)
    return response.content

# Define UI
chatbot = gr.ChatInterface(
    fn=rag_respond, # llama a la funcion rag_respond
    chatbot=gr.Chatbot(elem_id="chatbot",  height="auto", avatar_images=[USER_ICON, BOT_ICON]),
    title="RAG - Chatbot QA",
    textbox=gr.Textbox(placeholder=PLACE_HOLDER, container=True, scale=10),
    clear_btn=" Limpiar üóëÔ∏è",
    retry_btn=None,
    undo_btn=None,
    submit_btn="Enviar ‚òëÔ∏è",
    theme=gr.themes.Default(primary_hue="purple", secondary_hue="indigo"),
)

def main():
    chatbot.launch()
    #chatbot.launch(server_name="0.0.0.0", server_port=UI_PORT)

if __name__ == "__main__":
    logger.info("Running my app")
    sys.exit(main())